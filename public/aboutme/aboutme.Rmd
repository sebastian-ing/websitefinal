---
categories:
- "Social media"
date: "2020-10-14T22:26:09-05:00"
description: Analyzing how you can use boostrapping in R using data from social media survey
draft: false
image: socialmedia.jpg
keywords: ""
slug: confidence-intervals
title: Confidence intervals using boostrapping and formulas
---

```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)


```

<style>
table {
  background-color: white !important;
  color: black !important;
}
</style>

# Bootstrap method

The way I initially learned to do confidence intervals for populations was to use the formula. It is a (somewhat) complicated formula, where you would take a point estimate (often the mean), adding and subtracting some margin of error. The margin of error depends on the standard deviation (the higher the standard deviation, the higher the standard error), how confident we want to be in the interval (a 99% confidence interval will have a higher standard error than a 95%), and the number of observations (if we have a lot of data points, we can be more sure). 

However, another way to find a confidence interval is using the bootstrap method. This approach is more inductive rather than the formula which is more deductive. Using boostraping we have a sample and we find random observations, note them down, and do that over and over (we can pick the same observations). After that we can look at a distribution of the means we have found and create a 95% confidence interval where we see 95% of the observations.

I will show how both the formula and the boostrapping method can be used to construct intervals. To help us we will use the [General Social Survey (GSS)](http://www.gss.norc.org/), which gathers data on American society in order to monitor and explain trends in attitudes, behaviours, and attributes. Many trends have been tracked for decades, so one can see the evolution of attitudes, etc in American Society.
We will use the following packages (infer being particularly important as this allows us to do boostrapping):

```{r load-libraries}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(infer)
library(scales)
library(RColorBrewer)
library(knitr)
library(kableExtra)
library(countrycode)
```

## Data cleaning

In this project we analyze data from the **2016 GSS sample data**, using it to estimate values of *population parameters* of interest about US adults. The GSS sample data file has 2867 observations of 935 variables, but we are only interested in very few of these variables and we are therefore using a smaller file.


```{r, read_gss_data, cache=TRUE}
gss <- read_csv(here::here("data", "smallgss2016.csv"), 
               # We specify what will be NA values
                 na = c("", "Don't know",
                       "No answer", "Not applicable"))
```


Notice that many responses should not be taken into consideration, like "No Answer", "Don't Know", "Not applicable", "Refused to Answer".

We will be creating 95% confidence intervals for population parameters. The variables we have are the following:

- hours and minutes spent on email weekly. The responses to these questions are recorded in the `emailhr` and `emailmin` variables. For example, if the response is 2.50 hours, this would be recorded as emailhr = 2 and emailmin = 30.
- `snapchat`, `instagrm`, `twitter`: whether respondents used these social media in 2016
- `sex`: Female - Male
- `degree`: highest education level attained

## Instagram and Snapchat, by sex

Let us attempt to estimate the *population* proportion of Snapchat or Instagram users in 2016.

We will do this by:

1. Creating a  new variable, `snap_insta` that is *Yes* if the respondent reported using any of Snapchat (`snapchat`) **OR** Instagram (`instagrm`), and *No* if not. If the recorded value was NA for both of these questions, the value in the new variable should also be NA.

```{r snapinstanewcolumn}
# First we add a new column called snap_insta where if either snap or insta is true, we put "Yes", if both are no we put "No", and otherwise "NA"
Gss_mod <- gss %>%
  mutate(snap_insta = case_when(
    snapchat == "Yes" | instagrm == "Yes" ~ "Yes", 
    snapchat == "No" & instagrm == "No" ~ "No", 
    TRUE ~ "NA"))
```


1. Calculating the proportion of Yes’s for `snap_insta` among those who answered the question, i.e. excluding NAs.

```{r}
# We calculate the proportion
proportion_gss <- Gss_mod %>%
  #We get rid of NA values
  filter(snap_insta != "NA") %>% 
  group_by(snap_insta) %>%
  #We find number of observations for "Yes" and "No"
  count() %>%
  ungroup() %>% 
  mutate(proportion = n/sum(n)) %>% 
  # Fixes table
kable(caption = "TABLE 1.1: Proportion of people using snapchat or instagram",
             col.names = c("Uses snapchat or instagram?", "Observations", "Proportion")) %>% 
  kable_classic("basic", full_width = FALSE) 

proportion_gss
```


1. Using the CI formula for proportions, we will construct 95% CIs for men and women who used either Snapchat or Instagram

```{r instasnap}

# Now we do the same as above but for women and men
proportion_ci <- Gss_mod %>%
  filter(snap_insta != "NA") %>% 
  group_by(snap_insta, sex) %>%
  count()  %>% 
  group_by(sex) %>% 
  mutate(proportion = n / sum(n)) %>% 
  # Finally we only show values for yes
  filter(snap_insta == "Yes") %>% 
  # We summarise and calculate CI for the proportion
  summarise(proportion = proportion,
            se_proportion = sqrt(proportion * (1 - proportion) / n), 
              margin_of_error = 1.96* se_proportion, 
              CI_low = proportion - margin_of_error, 
              CI_high = proportion + margin_of_error
            ) %>% 
  kable(caption = "TABLE 1.2: Proportion of people using snapchat or instagram based on gender",
             col.names = c("Gender", "Proportion of yes", "Standard error", "margin of error", "Lower CI", "Upper CI")) %>% 
  kable_classic("striped", full_width = FALSE)

proportion_ci

```

There appears to be a larger share of females relative to males using instagram or snapchat, but given the two confidence intervals overlap, we are unable to make a definite conclusion on whether there is a signifianct difference between the two.


## Twitter, by education level

Let us now try to estimate the *population* proportion of Twitter users by education level in 2016. 

There are 5 education levels in variable `degree` which, in ascending order of years of education, are Lt high school, High School, Junior college, Bachelor, Graduate. Let us now go through a couple of steps to try and estimate the population proportion

1. Turn `degree` from a character variable into a factor variable. Make sure the order is the correct one and that levels are not sorted alphabetically which is what R by default does. 

```{r}
Gss_mod2 <- Gss_mod 
# Changes degree into factor
Gss_mod2$degree <- as.factor(Gss_mod2$degree) %>%  
  #reorders factor
  factor(levels = c("Lt high school", "High school", "Junior college", "Bachelor", "Graduate", "NA")) 


```





1. Create a  new variable, `bachelor_graduate` that is *Yes* if the respondent has either a `Bachelor` or `Graduate` degree. As before, if the recorded value for either was NA, the value in your new variable should also be NA.

```{r bachelor_graduate}
Gss_mod2 <- Gss_mod2 %>%
  mutate(bachelor_graduate = case_when(
    # Case when column should return yes
    degree == "Bachelor" | degree == "Graduate" ~ "Yes",
    #Case when column should return no
    degree == "High school" | degree == "Junior college" | degree == "Lt high school" ~ "No", 
    # Case when column should return NA (all remaining values)
    TRUE ~ "NA"))
```


1. Calculate the proportion of `bachelor_graduate` who do (Yes) and who don't (No) use twitter. 

```{r}
proportion_gss2 <- Gss_mod2 %>%
  # Removes NA observations
  filter(bachelor_graduate != "NA",
         twitter != "NA") %>% 
  #calculates proportions 
  group_by(bachelor_graduate, twitter) %>%
  #We count the observatoins
  count() %>%
  ungroup() %>% 
  group_by(bachelor_graduate) %>% 
  # We find the proportion
  mutate(proportion2 = n/sum(n)) %>%
  #We will only look at bachelor and graduates
  filter(bachelor_graduate != "No") %>%
  #fixes aesthetics
  kable(caption = "TABLE 1.3: Proportion of people using twitter with bachelor or graduate degrees",
            col.names = c("Bachelor or graduate degree?", "Twitter user", "Observations", "Proportion")) %>% 
  kable_classic("striped", full_width = FALSE)

proportion_gss2

```



1. Using the CI formula for proportions, let us construct two 95% CIs for `bachelor_graduate` vs whether they use (Yes) and don't (No) use twitter. 

```{r bachelor_graduate_prop}
proportion_ci2 <- Gss_mod2 %>%
  #Removes NA
  filter(bachelor_graduate != "NA",
         twitter != "NA") %>% 
  # Calculates n
  group_by(bachelor_graduate, twitter) %>%
  count()  %>% 
  ungroup() %>%
  group_by(bachelor_graduate) %>% 
  mutate(proportion2 = n/sum(n)) %>%
  filter(bachelor_graduate != "No") %>%
  # calculates proportion and elements in the CI formula
  group_by(bachelor_graduate) %>%  
  summarise(twitter = twitter,
            proportion2 = proportion2,
            se_proportion2 = sqrt(proportion2 * (1 - proportion2) / n), 
              margin_of_error2 = 1.96 * se_proportion2, 
              rating_low2 = proportion2 - margin_of_error2, 
              rating_high2 = proportion2 + margin_of_error2) %>% 
  #fixes aesthetics
  kable(caption = "TABLE 2.4: Proportion and confidence intervals of people using twitter based on education",
             col.names = c("Bachelor or graduate degree?", "Twitter user", "Proportion", "Standard error", "Margin of error", "Lower CI", "Upper CI")) %>% 
  kable_classic("striped", full_width = FALSE)
proportion_ci2
```

5. Do these two Confidence Intervals overlap?

They do not overlap and consequently we can conclude with 95% confidence that there is a significant difference in the population between people with bachelor's or graduate degrees using twitter, being that a majority does not use twitter.

## Email usage

Let us now try to estimate *population* parameter on time spent on email weekly. We do this through the following steps:

1. Create a new variable called `email` that combines `emailhr` and `emailmin` to reports the number of minutes the respondents spend on email weekly.

```{r email formating}
gss_email <- gss

# we change variables for emailhr and emailmin to numeric
gss_email$emailhr <- as.numeric(gss_email$emailhr)
gss_email$emailmin <- as.numeric(gss_email$emailmin)

# We create a combined column for email with data in minutes
gss_email <- gss_email %>%
  mutate(email = emailhr * 60 + emailmin)



```


1. Visualise the distribution of this new variable. Find the mean and the median number of minutes respondents spend on email weekly.

```{r email density graph}
#Let us first quickly see the mean and median values
gss_email %>% 
  #Find median and mean
  summarize(mean_email = mean(email, na.rm = TRUE),
            median_email = mean(email, na.rm = TRUE))
# Let us plot it
ggplot(gss_email, aes(x = email))+
  #Density graph
  geom_density()+
  #Vertical lines showing mean and median
  geom_vline(aes(xintercept = median(email, na.rm = TRUE)), color = "Blue", size = 0.5)+
  geom_vline(aes(xintercept = mean(email, na.rm = TRUE)), color = "Red", size = 0.5)+
  theme_bw()+
  labs(title = "Distribution of average minutes spent on emails \nper week",
       subtitle = "Blue line shows median; red line shows mean",
       x = "Minutes spent on email per week",
       y = "Proportion of observations")+
  #We make sure to show the value on the mean and median on the x/axis
  scale_x_continuous(breaks = c(median(gss_email$email, na.rm = TRUE), mean(gss_email$email, na.rm = TRUE), 1000, 2000, 3000, 4000, 5000))+
  theme(axis.text = element_text(size = 7))



```
We see the distribution is very skewed to the left. There is several outliers, who are spending an extremely high amount of time on emails, which means that the average of the distribution is affected significantly. The median is therefore a better measure for what a "typical" American would spend on e-mail per week.


1. Using the `infer` package, calculate a 95% bootstrap confidence interval for the mean amount of time Americans spend on email weekly. Interpret this interval in context of the data, reporting its endpoints in “humanized” units (e.g. instead of 108 minutes, report 1 hr and 8 minutes). If you get a result that seems a bit odd, discuss why you think this might be the case.

```{r}
# sets seed for replicability - this means you will be able to do the same analysis I did
set.seed(31)

gss_email_boot <- gss_email %>% 
  filter(email != "NA") %>% 
  # specifies variable we are interested in
  specify(response = email) %>% 
  # generates data using bootstrapping method
  generate(reps = 1000, 
           type = "bootstrap") %>% 
  # calculates the mean
  calculate(stat = "mean")

# calculates the confidence interval
ci_email <- gss_email_boot %>% 
  get_confidence_interval(level = 0.95, 
                          type = "percentile") %>% 
  # We change the variables to hrs and minutes by adding two new columns per CI
mutate(lower_ci_hour = lower_ci %/% 60,
       lower_ci_min = round(lower_ci %% 60, 0),
       upper_ci_hour = upper_ci %/% 60,
       upper_ci_min = round(upper_ci %% 60),0) %>% 
  # we then concatenate so we get the data in HH:MM format
  unite(col = "lower_ci_hhmm", lower_ci_hour, lower_ci_min, sep = ":") %>%
  unite(col = "upper_ci_hhmm", upper_ci_hour, upper_ci_min, sep = ":") %>% 
  # We remove all but these two new columns 
  select(lower_ci_hhmm, upper_ci_hhmm) %>% 
  # Table format
  kable(caption = "TABLE 2.5: Confidence interval for time spent on emails per week",
             col.names = c("Lower CI", "Upper CI")) %>% 
  kable_classic("striped", full_width = TRUE)


ci_email
```
This fits fairly well with the mean of 417 minutes (6:53), as it is in the interval



1. What would happen if we wanted a 99% confidence interval?

Given we have the same number of observations, n, and we want to say with higher certainty where the true value lies within, it follows we must then expand the range. Imagine an extreme case where we want to say with 100% certainty where the true value lies. To do this we must technically include all possible values. As we then decrease our degree of certainty, it follows that our interval also decreases, and consequently a 99% confidence interval will be larger than a 95% confidence interval.

## Conclusion

This project showed how you can use both bootstrapping and formulas to calculate the confidence interval of the mean / proportions.